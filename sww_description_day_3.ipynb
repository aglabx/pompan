{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "113f2c10",
   "metadata": {},
   "source": [
    "# Поиск генов под отбором"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402093b",
   "metadata": {},
   "source": [
    "**1. Подготовка исходных данных.**  \n",
    "В качестве исходных данных нам нужно три организма, для которых есть геномные сборки (.fasta/.fna), аннотация (.gff) и белки (.fasta/.faa)\n",
    "\n",
    "Предположим, что они у нас называются genome1.fna, annot1.gff и protein1.faa для первого организма и т.д. по аналогии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610986f9",
   "metadata": {},
   "source": [
    "**2. Кластеризация белков с целью обнаружения однокопийных ортологов.**  \n",
    "Установка программы:  \n",
    "conda install proteinortho  \n",
    "\n",
    "Запуск кластеризации:  \n",
    "proteinortho6.pl protein1.faa protein2.faa protein3.faa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087be18",
   "metadata": {},
   "source": [
    "Эта команда выдаст файл my-project.proteinortho.tsv, содержащий все ортологичные группы. Чтобы вытащить только интересующие нас однокопийные ортологи (т.е. гены, в каждом организме присутвующие только в одном экземпляре), удобно воспользоваться awk:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c073ccf0",
   "metadata": {},
   "source": [
    "awk -F'\\t' '{if ($2=='3') if ($1=='3') print $0}' my-project.proteinortho.tsv > single_copy.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ee22b",
   "metadata": {},
   "source": [
    "**3. Подготовка данных для программы PAML**  \n",
    "По айдишкам из файла single_copy.tsv необходимо вытащить соответствующие этим айдишникам белковые и кодирующие нуклеотидные последовательности (CDS).\n",
    "\n",
    "На самом деле эта самая геморройная задача этого проекта, но я надеюсь, что вы справитесь :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ebda25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def get_revcomp(sequence):\n",
    "    '''Return reverse complementary sequence.\n",
    "    >>> complementary('AT CG')\n",
    "    'CGAT'\n",
    "    '''\n",
    "    c = dict(zip('ATCGNatcgn~[]', 'TAGCNtagcn~]['))\n",
    "    return ''.join(c.get(nucleotide, '') for nucleotide in reversed(sequence))\n",
    "\n",
    "def read_fasta_file(fasta_file):              \n",
    "    fasta = {}                                \n",
    "    header = None                            \n",
    "    with open(fasta_file) as fh:              \n",
    "        for i, line in enumerate(fh):         \n",
    "            line = line.strip()               \n",
    "            if line.startswith(\">\"):          \n",
    "                if header:                   \n",
    "                    fasta[header] = \"\".join(seq)\n",
    "                header = line[1:].split()[0]         \n",
    "                seq = []                      \n",
    "            else:\n",
    "                seq.append(line)              \n",
    "        if header:                            \n",
    "            fasta[header] = \"\".join(seq)      \n",
    "    return fasta\n",
    "\n",
    "\n",
    "def gtf2seq(gtf_file, fasta_dict, is_ncbi=False):\n",
    "    cds_seq = defaultdict(list)\n",
    "    with open(gtf_file) as fh:\n",
    "        for line in fh:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\"\\t\")\n",
    "\n",
    "            if line[2] != \"CDS\":\n",
    "                continue\n",
    "            \n",
    "            scaf_id = line[0]\n",
    "            if is_ncbi:\n",
    "                gene_name = re.findall(\"Name=(\\S+?);\", line[-1])\n",
    "                if not gene_name:\n",
    "                    print(line)\n",
    "                    continue\n",
    "                gene_name = gene_name[0]\n",
    "            else:\n",
    "                gene_name = line[-1].split()[1].replace('\"', \"\").replace(';', \"\") \n",
    "            strand = line[6]\n",
    "            if strand == \"+\":\n",
    "                cds_start = int(line[3])-1\n",
    "                cds_end = int(line[4])\n",
    "            else:\n",
    "                \n",
    "            \n",
    "            key = (gene_name, strand)\n",
    "            \n",
    "            if cds_start > cds_end:\n",
    "                cds_start, cds_end = cds_end, cds_start ### order them!\n",
    "            exon = fasta_dict[scaf_id][cds_start:cds_end] \n",
    "            value = (cds_start, exon)\n",
    "            cds_seq[key].append(value)\n",
    "            \n",
    "    genes = {}\n",
    "    for key in cds_seq:\n",
    "        (gene_name, strand) = key\n",
    "        exons = cds_seq[key]\n",
    "        exons.sort() ### ну вдруг у тебя там координаты сломались в gtf/gff, оно бывает, а так мы уверены что они по порядку будут\n",
    "        gene = \"\".join([x[1] for x in exons]) # тут мы просто берем сиквенсы и скипаем служебные старты для сортировки\n",
    "        if strand == \"-\":\n",
    "            gene = get_revcomp(gene)\n",
    "        ### параноидально чекаем что у нас gene_name уникальное, если тут упадет, то у тебя паралоги собираются по имени в одну корзинку\n",
    "        assert gene_name not in genes\n",
    "        genes[gene_name] = gene\n",
    "    return genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69891cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пропишите тут все пути к соответствующим файлам\n",
    "\n",
    "sngl_copy = \"/path/to/file\"\n",
    "\n",
    "genome1_file = \"/path/to/file\"\n",
    "annot1_file = \"/path/to/file\"\n",
    "protein1_file = \"/path/to/file\"\n",
    "\n",
    "genome2_file = \"/path/to/file\"\n",
    "annot2_file = \"/path/to/file\"\n",
    "protein2_file = \"/path/to/file\"\n",
    "\n",
    "genome3_file = \"/path/to/file\"\n",
    "annot3_file = \"/path/to/file\"\n",
    "protein3_file = \"/path/to/file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97799eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_165610/2570592904.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenome1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_fasta_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenome1_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgenome2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_fasta_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenome2_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgenome3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_fasta_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenome3_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprotein1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_fasta_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein1_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_165610/2263189793.py\u001b[0m in \u001b[0;36mread_fasta_file\u001b[0;34m(fasta_file)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfasta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasta_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/file'"
     ]
    }
   ],
   "source": [
    "# cчитываем из файлов разные типы данных и \n",
    "# сохраняем их в виде словарей\n",
    "\n",
    "genome1 = read_fasta_file(genome1_file)\n",
    "genome2 = read_fasta_file(genome2_file)\n",
    "genome3 = read_fasta_file(genome3_file)\n",
    "\n",
    "protein1 = read_fasta_file(protein1_file)\n",
    "protein2 = read_fasta_file(protein2_file)\n",
    "protein3 = read_fasta_file(protein3_file)\n",
    "\n",
    "# если геном не в формате с NCBI, то не забудьте убрать аргумент is_ncbi\n",
    "\n",
    "cds1 = gff2seq(annot1_file, genome1, is_ncbi=False)\n",
    "cds2 = gff2seq(annot2_file, genome2, is_ncbi=False)\n",
    "cds3 = gff2seq(annot3_file, genome3, is_ncbi=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## дальше довольно кривой код, feel free to привести его в порядок\n",
    "\n",
    "## каждый кластер однокопийных ортологов мы записываем в отдельный файл\n",
    "\n",
    "## обязательно нужно знать порядок столбцов в файле single_copy.txt\n",
    "\n",
    "# их нужно сохранить в отдельную папку cds_clusters (не забудьте ее создать!)\n",
    "\n",
    "counter = 0\n",
    "with open(sngl_copy) as orthologs:\n",
    "    for line in orthologs:\n",
    "        file = \"/path/to/output/files/cds_clusters/cluster_%s.dna.mfa\" % str(counter)\n",
    "        counter += 1\n",
    "        with open(file, \"w\") as fh:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split()\n",
    "\n",
    "#             print(len(cds2[line[3]]))\n",
    "#             print(len(cds2[line[4]]))\n",
    "#             print(len(cds3[line[5]]))\n",
    "\n",
    "            fh.write(\">organism1_\" + line[3] + \"\\n\")\n",
    "            fh.write(cds1[line[3]] + \"\\n\")\n",
    "            fh.write(\">organism2_\" + line[4] + \"\\n\")\n",
    "            fh.write(cds2[line[4]] + \"\\n\")\n",
    "            fh.write(\">organism3_\" + line[5] + \"\\n\")\n",
    "            fh.write(cds3[line[5]] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# то же самое проделываем для белковых кластеров\n",
    "\n",
    "# их нужно сохранить в отдельную папку prot_clusters (не забудьте ее создать!)\n",
    "\n",
    "counter = 0\n",
    "with open(sngl_copy) as orthologs:\n",
    "    for line in orthologs:\n",
    "        file = \"/path/to/output/files/prot_clusters/cluster_%s.prot.mfa\" % str(counter)\n",
    "        counter += 1\n",
    "        with open(file, \"w\") as fh:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split()\n",
    "\n",
    "#             print(len(protein1[line[3]]))\n",
    "#             print(len(protein2[line[4]]))\n",
    "#             print(len(protein3[line[5]]))\n",
    "\n",
    "            fh.write(\">organism1_\" + line[3] + \"\\n\")\n",
    "            fh.write(cds1[line[3]] + \"\\n\")\n",
    "            fh.write(\">organism2_\" + line[4] + \"\\n\")\n",
    "            fh.write(cds2[line[4]] + \"\\n\")\n",
    "            fh.write(\">organism3_\" + line[5] + \"\\n\")\n",
    "            fh.write(cds3[line[5]] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d409369",
   "metadata": {},
   "source": [
    "Теперь у нас есть папки с распарсенными кодирующими последовательностями и белками. Следующий шаг - выравнивание."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14620f",
   "metadata": {},
   "source": [
    "**4. Выравнивание белковой последовательности.**\n",
    "\n",
    "Чтобы получить выравнивание по кодонам, сначала выравнивают белки между собой в одном кластере. Почему именно белки, а не CDS, - они более консервативные, последовательности гораздо легче выравнять правильно\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a8792a",
   "metadata": {},
   "source": [
    "Запустила команды с clustal omega для того, чтобы выровнять каждый кластер белка.  \n",
    "clustalo -i {input_file} -o {output_file}\n",
    "\n",
    "Для распараллеливания большого количества:  \n",
    "mkdir /path/to/output/files/prot_clusters/aligned/\n",
    "\n",
    "ls /path/to/output/files/prot_clusters/ | grep mfa | xargs -I {} -P 120 sh -c \"clustalo -i /path/to/output/files/prot_clusters/{} -o /path/to/output/files/prot_clusters/aligned/{}.aligned\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878d9ff",
   "metadata": {},
   "source": [
    "**3. Выравнивание по кодонам**\n",
    "\n",
    "Далее используем pal2nal, он переводит белковое выравнивание в выравнивание по кодонами + убирает все гэпы (потому что они создают false positive для PAML) + переводит выравнивание в фoрмат, который хочет PAML)  \n",
    "pal2nal.pl cluster_1.prot.mfa.aligned cluster_1.dna.mfa -output paml -nogap > cluster_1.pal2nal  \n",
    "\n",
    "Но так как у нас есть несколько сотен файлов, нужно распараллеливать. Для этого нам нужен файл с записью всех комманд:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec22eea",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_165610/2815066985.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msngl_copy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0morthologs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/mnt/projects/african_parrots/sochkalova/diploma/orthologs2/tasks.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morthologs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cluster_%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/file'"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "with open(sngl_copy) as orthologs:\n",
    "    with open(\"tasks.txt\", \"w\") as fh:\n",
    "        for line in orthologs:\n",
    "            file = \"cluster_%s\" % str(counter)\n",
    "            counter += 1\n",
    "\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "                \n",
    "            command = f\"pal2nal.pl prot_clusters/aligned/{file}.prot.mfa.aligned cds_clusters/{file}.dna.mfa -output paml -nogap > pal2nal/{file}.pal2nal\"\n",
    "            fh.write(command + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd0fdf",
   "metadata": {},
   "source": [
    "Для распараллеливания большого количества:  \n",
    "less tasks.txt | xargs -I {} -P 120 sh -c \"{}\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "7512fa15",
   "metadata": {},
   "source": [
    "Из-за косяков внутри скриптов и проблем из-за работы с сотнями файлов одновременно, могут происходить баги. Это приводит к тому, что часть файлов в папке pal2nal будет пустыми. \n",
    "Посмотреть пустые файлы директории:\n",
    "\n",
    "ls -lah pal2nal/ | awk  '{if ($5=='0')  print $0}'\n",
    "\n",
    "Посчитать их количество:\n",
    "ls -lah pal2nal/ | awk  '{if ($5=='0')  print $0}' | wc -l\n",
    "\n",
    "С этими багами можно провозиться очень долго, поэтому предлагаю вам их просто выкинуть из рассмотрения (удаляйте их нахрен)\n",
    "\n",
    "ls -lah pal2nal/ | awk  '{if ($5=='0')  print $0}' | xargs -I {} -P 10 sh -c \"rm pal2nal/{}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5cdff",
   "metadata": {},
   "source": [
    "Когда все упомянутое сделано, можно запускать PAML\n",
    "\n",
    "Мануал: DOI: 10.1007/978-1-4939-1438-8_4 \n",
    "\n",
    "Если успеетее сделать все вышеописанное за сегодняшний день, я скину инструкцию по PAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda3ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
